{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":116062,"databundleVersionId":14084779,"sourceType":"competition"},{"sourceId":5499219,"sourceType":"datasetVersion","datasetId":3167603},{"sourceId":5549164,"sourceType":"datasetVersion","datasetId":3197305},{"sourceId":5607816,"sourceType":"datasetVersion","datasetId":3225525},{"sourceId":5792099,"sourceType":"datasetVersion","datasetId":3327296},{"sourceId":6247561,"sourceType":"datasetVersion","datasetId":3590060}],"dockerImageVersionId":31154,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# ============================================================================\n# PACKAGE INSTALLATION\n# ============================================================================\n# Install any missing packages\n!pip install -q obonet biopython","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-14T08:33:44.099551Z","iopub.execute_input":"2025-12-14T08:33:44.099885Z","iopub.status.idle":"2025-12-14T08:33:49.885049Z","shell.execute_reply.started":"2025-12-14T08:33:44.099848Z","shell.execute_reply":"2025-12-14T08:33:49.884238Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m33.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25h","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\nimport numpy as np\nimport pandas as pd\nimport os\n\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# ============================================================================\n# CORE IMPORTS\n# ============================================================================\nimport gc\nimport sys\nimport subprocess\nfrom pathlib import Path\nfrom collections import defaultdict, Counter\nfrom tqdm.auto import tqdm\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# ============================================================================\n# CONFIG (ADVANCED IMPROVEMENTS)\n# ============================================================================\nCONFIG = {\n    'MAX_TRAIN_SAMPLES': 90000,        # all data\n    'TOP_K_LABELS': 800,\n    'RANDOM_SEED': 253,\n    'HIDDEN_DIMS': [512, 256, 128],\n    'DROPOUT_RATE': 0.27,             # IMPROVED: 0.3 â†’ 0.27 (less dropout for more data)\n    'EPOCHS': 36,                     # IMPROVED: 12 - 36 epochs\n    'BATCH_SIZE': 64,                 # IMPROVED: 32 â†’ 64 (faster training)\n    'LEARNING_RATE': 1e-3,\n    'PREDICT_BATCH_SIZE': 128,        # IMPROVED: 64 â†’ 128 (faster prediction)\n    'MIN_CONFIDENCE': 0.05,           # IMPROVED: 0.10 â†’ 0.05 (even more predictions)\n    'MAX_PREDS_PER_PROTEIN': 200,    # IMPROVED: 150 â†’ 200\n    'LABEL_SMOOTHING': 0.08,          # NEW: Add label smoothing\n    'TEMPERATURE': 1.4,               # NEW: Temperature scaling for predictions\n    'MERGE_WITH_BLAST': True,\n    'BLAST_WEIGHT': 0.60,             \n    'DL_WEIGHT': 0.40,                \n    'BASE_PATH': '/kaggle/input/cafa-6-protein-function-prediction',\n    'ESM2_PATH': 'cafa-5-ems-2-embeddings-numpy',\n    'BLAST_PATH': '/kaggle/input/blast-quick-sprof-zero-pred/submission.tsv',\n}\n\nprint(\"=\"*80)\nprint(\"ðŸš€ CAFA 6 - ADVANCED OPTIMIZATION\")\nprint(\"=\"*80)\n\ndef install(package):\n    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", package])\n\ntry:\n    import torch\n    import torch.nn as nn\n    import torch.nn.functional as F\nexcept:\n    install('torch')\n    import torch\n    import torch.nn as nn\n    import torch.nn.functional as F\n\nnp.random.seed(CONFIG['RANDOM_SEED'])\ntorch.manual_seed(CONFIG['RANDOM_SEED'])\nif torch.cuda.is_available():\n    torch.cuda.manual_seed(CONFIG['RANDOM_SEED'])\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f\"Device: {device}\")\n\n# ============================================================================\n# LOAD DATA\n# ============================================================================\nBASE = Path(CONFIG['BASE_PATH'])\nTRAIN_DIR = BASE / 'Train'\n\nprint(\"\\n[1/5] Loading annotations...\")\ntrain_terms_df = pd.read_csv(TRAIN_DIR / 'train_terms.tsv', sep='\\t', \n                              header=None, names=['protein', 'term', 'ontology'])\nprotein_to_terms = train_terms_df.groupby('protein')['term'].apply(list).to_dict()\n\nterm_counts = Counter()\nfor terms in protein_to_terms.values():\n    term_counts.update(terms)\ntop_terms = [t for t, _ in term_counts.most_common(CONFIG['TOP_K_LABELS'])]\n\nfor protein in protein_to_terms:\n    protein_to_terms[protein] = [t for t in protein_to_terms[protein] if t in top_terms]\n\nterm_to_idx = {term: idx for idx, term in enumerate(top_terms)}\nprint(f\"   âœ“ {len(protein_to_terms)} proteins, {len(top_terms)} terms\")\n\nprint(\"\\n[2/5] Loading ESM2 embeddings...\")\nesm2_base = f\"/kaggle/input/{CONFIG['ESM2_PATH']}\"\ntrain_ids = np.load(f\"{esm2_base}/train_ids.npy\", allow_pickle=True)\ntrain_embeds = np.load(f\"{esm2_base}/train_embeddings.npy\")\ntest_ids = np.load(f\"{esm2_base}/test_ids.npy\", allow_pickle=True)\ntest_embeds = np.load(f\"{esm2_base}/test_embeddings.npy\")\n\ntrain_dict = {str(pid): emb for pid, emb in zip(train_ids, train_embeds)}\ntest_dict = {str(pid): emb for pid, emb in zip(test_ids, test_embeds)}\nembed_dim = train_embeds.shape[1]\nprint(f\"   âœ“ Dim: {embed_dim}\")\n\ndel train_ids, train_embeds, test_ids, test_embeds\ngc.collect()\n\nprint(\"\\n[3/5] Preparing training data...\")\nvalid_proteins = [p for p in protein_to_terms.keys() if p in train_dict][:CONFIG['MAX_TRAIN_SAMPLES']]\n\nfrom sklearn.preprocessing import MultiLabelBinarizer\nfrom sklearn.model_selection import train_test_split\n\nmlb = MultiLabelBinarizer(classes=range(len(top_terms)))\ny_labels = [[term_to_idx[t] for t in protein_to_terms.get(p, []) if t in term_to_idx] \n            for p in valid_proteins]\ny_encoded = mlb.fit_transform(y_labels)\n\n# Apply label smoothing\nif CONFIG['LABEL_SMOOTHING'] > 0:\n    y_encoded = y_encoded.astype(float)\n    y_encoded = y_encoded * (1 - CONFIG['LABEL_SMOOTHING']) + CONFIG['LABEL_SMOOTHING'] / len(top_terms)\n\ntrain_proteins, val_proteins, y_train, y_val = train_test_split(\n    valid_proteins, y_encoded, test_size=0.15, random_state=CONFIG['RANDOM_SEED']\n)\nprint(f\"   âœ“ Train: {len(train_proteins)}, Val: {len(val_proteins)}\")\n\n# ============================================================================\n# MODEL\n# ============================================================================\nprint(\"\\n[4/5] Building model...\")\n\nclass ImprovedProteinModel(nn.Module):\n    \"\"\"Improved 3-layer architecture with BatchNorm\"\"\"\n    def __init__(self, input_dim, output_dim, hidden_dims, dropout):\n        super().__init__()\n        layers = []\n        prev_dim = input_dim\n        for dim in hidden_dims:\n            layers.extend([\n                nn.Linear(prev_dim, dim),\n                nn.BatchNorm1d(dim),\n                nn.ReLU(),\n                nn.Dropout(dropout)\n            ])\n            prev_dim = dim\n        self.encoder = nn.Sequential(*layers)\n        self.output = nn.Linear(prev_dim, output_dim)\n    \n    def forward(self, x):\n        return torch.sigmoid(self.output(self.encoder(x)))\n\nmodel = ImprovedProteinModel(embed_dim, len(top_terms), CONFIG['HIDDEN_DIMS'], CONFIG['DROPOUT_RATE']).to(device)\nprint(f\"   âœ“ Parameters: {sum(p.numel() for p in model.parameters()):,}\")\n\n# ============================================================================\n# TRAINING\n# ============================================================================\nprint(\"\\n\" + \"=\"*80)\nprint(\"TRAINING (12 EPOCHS WITH LABEL SMOOTHING)\")\nprint(\"=\"*80)\n\ncriterion = nn.BCELoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=CONFIG['LEARNING_RATE'], weight_decay=1e-5)  # Added L2 regularization\nscheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=2, verbose=False)\n\nbest_val_loss = float('inf')\n\nfor epoch in range(CONFIG['EPOCHS']):\n    model.train()\n    indices = np.random.permutation(len(train_proteins))\n    epoch_loss = 0\n    n_batches = 0\n    \n    for i in range(0, len(indices), CONFIG['BATCH_SIZE']):\n        batch_idx = indices[i:i + CONFIG['BATCH_SIZE']]\n        batch_proteins = [train_proteins[j] for j in batch_idx]\n        \n        X_batch = torch.FloatTensor([train_dict[p] for p in batch_proteins]).to(device)\n        y_batch = torch.FloatTensor(y_train[batch_idx]).to(device)\n        \n        optimizer.zero_grad()\n        outputs = model(X_batch)\n        loss = criterion(outputs, y_batch)\n        loss.backward()\n        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)  # Gradient clipping\n        optimizer.step()\n        \n        epoch_loss += loss.item()\n        n_batches += 1\n    \n    model.eval()\n    val_loss = 0\n    val_batches = 0\n    \n    with torch.no_grad():\n        for i in range(0, len(val_proteins), CONFIG['BATCH_SIZE']):\n            batch_proteins = val_proteins[i:i + CONFIG['BATCH_SIZE']]\n            X_batch = torch.FloatTensor([train_dict[p] for p in batch_proteins]).to(device)\n            y_batch = torch.FloatTensor(y_val[i:i + CONFIG['BATCH_SIZE']]).to(device)\n            \n            outputs = model(X_batch)\n            loss = criterion(outputs, y_batch)\n            val_loss += loss.item()\n            val_batches += 1\n    \n    train_loss_avg = epoch_loss/n_batches\n    val_loss_avg = val_loss/val_batches\n    \n    scheduler.step(val_loss_avg)\n    \n    if val_loss_avg < best_val_loss:\n        best_val_loss = val_loss_avg\n        print(f\"Epoch {epoch+1}: Train={train_loss_avg:.4f}, Val={val_loss_avg:.4f} â­ NEW BEST\")\n    else:\n        print(f\"Epoch {epoch+1}: Train={train_loss_avg:.4f}, Val={val_loss_avg:.4f}\")\n    \n    gc.collect()\n\nprint(f\"\\nâœ… Best Validation Loss: {best_val_loss:.4f}\")\n\n# ============================================================================\n# PREDICTIONS (WITH TEMPERATURE SCALING)\n# ============================================================================\nprint(\"\\n\" + \"=\"*80)\nprint(\"PREDICTIONS (WITH TEMPERATURE SCALING)\")\nprint(\"=\"*80)\n\nmodel.eval()\ntest_protein_ids = list(test_dict.keys())\n\nn_predictions = 0\nwith open('temp_dl.tsv', 'w', newline='') as f:\n    with torch.no_grad():\n        for start in tqdm(range(0, len(test_protein_ids), CONFIG['PREDICT_BATCH_SIZE']), desc=\"Predicting\"):\n            batch_ids = test_protein_ids[start:start + CONFIG['PREDICT_BATCH_SIZE']]\n            X_batch = torch.FloatTensor([test_dict[p] for p in batch_ids]).to(device)\n            \n            # Get raw logits before sigmoid\n            logits = model.encoder(X_batch)\n            logits = model.output(logits)\n            \n            # Apply temperature scaling\n            outputs = torch.sigmoid(logits / CONFIG['TEMPERATURE']).cpu().numpy()\n            \n            for i, pid in enumerate(batch_ids):\n                probs = outputs[i]\n                top_indices = np.argsort(probs)[::-1][:CONFIG['MAX_PREDS_PER_PROTEIN']]\n                confident_indices = [idx for idx in top_indices if probs[idx] > CONFIG['MIN_CONFIDENCE']]\n                \n                for idx in confident_indices:\n                    line = f\"{pid}\\t{top_terms[idx]}\\t{min(probs[idx], 0.999):.3f}\\n\"\n                    f.write(line)\n                    n_predictions += 1\n            \n            del X_batch, outputs, logits\n            if start % 1000 == 0:\n                gc.collect()\n\nprint(f\"âœ“ Generated {n_predictions:,} predictions\")\n\ndel model, train_dict\ngc.collect()\n\n# ============================================================================\n# ENSEMBLE\n# ============================================================================\nprint(\"\\n\" + \"=\"*80)\nprint(\"ENSEMBLE\")\nprint(\"=\"*80)\n\nif CONFIG['MERGE_WITH_BLAST'] and os.path.exists(CONFIG['BLAST_PATH']):\n    print(\"Merging with BLAST...\")\n    \n    dl_df = pd.read_csv('temp_dl.tsv', sep='\\t', header=None, names=['Id', 'GO term', 'Confidence'])\n    dl_df['Confidence'] = dl_df['Confidence'].astype(float) * CONFIG['DL_WEIGHT']\n    \n    blast_chunks = []\n    for chunk in pd.read_csv(CONFIG['BLAST_PATH'], sep='\\t', header=None,\n                             names=['Id', 'GO term', 'Confidence'], chunksize=1000000):\n        chunk['Confidence'] = chunk['Confidence'].astype(float) * CONFIG['BLAST_WEIGHT']\n        blast_chunks.append(chunk)\n        if len(blast_chunks) >= 10:\n            blast_df = pd.concat(blast_chunks, ignore_index=True)\n            blast_chunks = [blast_df]\n            gc.collect()\n    \n    blast_df = pd.concat(blast_chunks, ignore_index=True)\n    print(f\"  DL={len(dl_df):,}, BLAST={len(blast_df):,}\")\n    \n    ensemble_df = pd.concat([dl_df, blast_df], ignore_index=True)\n    del dl_df, blast_df, blast_chunks\n    gc.collect()\n    \n    ensemble_df = ensemble_df.groupby(['Id', 'GO term'], as_index=False)['Confidence'].sum()\n    ensemble_df = ensemble_df.sort_values(['Id', 'Confidence'], ascending=[True, False])\n    ensemble_df = ensemble_df.groupby('Id').head(1500)\n    \n    import csv\n    ensemble_df.to_csv('submission.tsv', sep='\\t', header=False, index=False,\n                      quoting=csv.QUOTE_NONE, escapechar='\\\\', lineterminator='\\n')\n    print(f\"âœ“ Saved {len(ensemble_df):,} predictions\")\n    \n    del ensemble_df\nelse:\n    os.rename('temp_dl.tsv', 'submission.tsv')\n\ngc.collect()\n\n# ============================================================================\n# CLEANUP\n# ============================================================================\nprint(\"\\n[CLEANUP]\")\nfor fname in os.listdir('.'):\n    if 'submission' in fname.lower() and fname != 'submission.tsv':\n        try:\n            os.remove(fname)\n            print(f\"  âœ— Removed {fname}\")\n        except:\n            pass\n\nif os.path.exists('temp_dl.tsv'):\n    os.remove('temp_dl.tsv')\n    print(f\"  âœ— Removed temp_dl.tsv\")\n\n# ============================================================================\n# VERIFICATION\n# ============================================================================\nprint(\"\\n\" + \"=\"*80)\nprint(\"âœ… VERIFICATION\")\nprint(\"=\"*80)\n\nif os.path.exists('submission.tsv'):\n    with open('submission.tsv', 'r') as f:\n        lines = f.readlines()\n    \n    n_lines = len(lines)\n    file_size_mb = os.path.getsize('submission.tsv') / (1024*1024)\n    \n    print(f\"ðŸ“‹ File: submission.tsv\")\n    print(f\"ðŸ“Š Lines: {n_lines:,}\")\n    print(f\"ðŸ’¾ Size: {file_size_mb:.1f} MB\")\n    \n    print(\"\\nâœ… First 5 lines:\")\n    for i in range(min(5, len(lines))):\n        line = lines[i].strip()\n        parts = line.split('\\t')\n        if len(parts) == 3:\n            print(f\"  âœ“ {parts[0]:<15} {parts[1]:<12} {parts[2]}\")\n        else:\n            print(f\"  âœ— Line {i+1}: {len(parts)} columns (expected 3)\")\n    \n    first_line = lines[0].strip()\n    parts = first_line.split('\\t')\n    \n    print(\"\\nðŸ” Format check:\")\n    all_good = True\n    \n    if len(parts) == 3:\n        print(f\"  âœ“ Columns: 3\")\n    else:\n        print(f\"  âœ— Columns: {len(parts)} (expected 3)\")\n        all_good = False\n    \n    if parts[1].startswith('GO:'):\n        print(f\"  âœ“ GO term format\")\n    else:\n        print(f\"  âœ— GO term: {parts[1]}\")\n        all_good = False\n    \n    try:\n        conf = float(parts[2])\n        if 0 < conf <= 1:\n            print(f\"  âœ“ Confidence: {conf}\")\n        else:\n            print(f\"  âœ— Confidence out of range: {conf}\")\n            all_good = False\n    except:\n        print(f\"  âœ— Confidence not a number: {parts[2]}\")\n        all_good = False\n    \n    if '\\n' in lines[0]:\n        print(f\"  âœ“ Newlines present\")\n    else:\n        print(f\"  âœ— No newlines!\")\n        all_good = False\n    \n    if all_good:\n        print(\"\\nâœ…âœ…âœ… FORMAT IS CORRECT! âœ…âœ…âœ…\")\n    else:\n        print(\"\\nâš ï¸  FORMAT HAS ISSUES!\")\n        \n    print(\"\\nðŸ“ Files in working directory:\")\n    for fname in sorted(os.listdir('.')):\n        if not fname.startswith('.'):\n            size = os.path.getsize(fname) / (1024*1024)\n            print(f\"  {fname} ({size:.1f} MB)\")\nelse:\n    print(\"âŒ ERROR: submission.tsv not found!\")\n\nprint(\"\\n\" + \"=\"*80)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-14T08:33:49.886996Z","iopub.execute_input":"2025-12-14T08:33:49.887234Z","iopub.status.idle":"2025-12-14T08:47:57.983088Z","shell.execute_reply.started":"2025-12-14T08:33:49.887213Z","shell.execute_reply":"2025-12-14T08:47:57.982373Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/blast-quick-sprof-zero-pred/submission.tsv\n/kaggle/input/protbert-embeddings-for-cafa5/train_ids.npy\n/kaggle/input/protbert-embeddings-for-cafa5/train_embeddings.npy\n/kaggle/input/protbert-embeddings-for-cafa5/test_ids.npy\n/kaggle/input/protbert-embeddings-for-cafa5/test_embeddings.npy\n/kaggle/input/train-targets-top500/train_targets_top500.npy\n/kaggle/input/cafa-5-ems-2-embeddings-numpy/train_ids.npy\n/kaggle/input/cafa-5-ems-2-embeddings-numpy/train_embeddings.npy\n/kaggle/input/cafa-5-ems-2-embeddings-numpy/test_ids.npy\n/kaggle/input/cafa-5-ems-2-embeddings-numpy/test_embeddings.npy\n/kaggle/input/cafa-6-protein-function-prediction/sample_submission.tsv\n/kaggle/input/cafa-6-protein-function-prediction/IA.tsv\n/kaggle/input/cafa-6-protein-function-prediction/Test/testsuperset.fasta\n/kaggle/input/cafa-6-protein-function-prediction/Test/testsuperset-taxon-list.tsv\n/kaggle/input/cafa-6-protein-function-prediction/Train/train_terms.tsv\n/kaggle/input/cafa-6-protein-function-prediction/Train/train_sequences.fasta\n/kaggle/input/cafa-6-protein-function-prediction/Train/train_taxonomy.tsv\n/kaggle/input/cafa-6-protein-function-prediction/Train/go-basic.obo\n/kaggle/input/t5embeds/train_ids.npy\n/kaggle/input/t5embeds/test_embeds.npy\n/kaggle/input/t5embeds/train_embeds.npy\n/kaggle/input/t5embeds/test_ids.npy\n================================================================================\nðŸš€ CAFA 6 - ADVANCED OPTIMIZATION\n================================================================================\nDevice: cuda\n\n[1/5] Loading annotations...\n   âœ“ 82405 proteins, 800 terms\n\n[2/5] Loading ESM2 embeddings...\n   âœ“ Dim: 1280\n\n[3/5] Preparing training data...\n   âœ“ Train: 67377, Val: 11891\n\n[4/5] Building model...\n   âœ“ Parameters: 925,088\n\n================================================================================\nTRAINING (12 EPOCHS WITH LABEL SMOOTHING)\n================================================================================\nEpoch 1: Train=0.0401, Val=0.0212 â­ NEW BEST\nEpoch 2: Train=0.0209, Val=0.0200 â­ NEW BEST\nEpoch 3: Train=0.0201, Val=0.0196 â­ NEW BEST\nEpoch 4: Train=0.0198, Val=0.0195 â­ NEW BEST\nEpoch 5: Train=0.0197, Val=0.0193 â­ NEW BEST\nEpoch 6: Train=0.0196, Val=0.0193\nEpoch 7: Train=0.0196, Val=0.0193 â­ NEW BEST\nEpoch 8: Train=0.0196, Val=0.0192 â­ NEW BEST\nEpoch 9: Train=0.0196, Val=0.0193\nEpoch 10: Train=0.0196, Val=0.0193\nEpoch 11: Train=0.0196, Val=0.0193\nEpoch 12: Train=0.0193, Val=0.0189 â­ NEW BEST\nEpoch 13: Train=0.0192, Val=0.0189 â­ NEW BEST\nEpoch 14: Train=0.0192, Val=0.0188 â­ NEW BEST\nEpoch 15: Train=0.0191, Val=0.0188 â­ NEW BEST\nEpoch 16: Train=0.0191, Val=0.0188\nEpoch 17: Train=0.0191, Val=0.0188 â­ NEW BEST\nEpoch 18: Train=0.0191, Val=0.0187 â­ NEW BEST\nEpoch 19: Train=0.0190, Val=0.0187 â­ NEW BEST\nEpoch 20: Train=0.0190, Val=0.0187 â­ NEW BEST\nEpoch 21: Train=0.0190, Val=0.0187\nEpoch 22: Train=0.0190, Val=0.0187 â­ NEW BEST\nEpoch 23: Train=0.0190, Val=0.0187\nEpoch 24: Train=0.0190, Val=0.0187\nEpoch 25: Train=0.0190, Val=0.0187\nEpoch 26: Train=0.0188, Val=0.0184 â­ NEW BEST\nEpoch 27: Train=0.0187, Val=0.0184 â­ NEW BEST\nEpoch 28: Train=0.0187, Val=0.0184\nEpoch 29: Train=0.0186, Val=0.0184 â­ NEW BEST\nEpoch 30: Train=0.0186, Val=0.0184\nEpoch 31: Train=0.0186, Val=0.0183 â­ NEW BEST\nEpoch 32: Train=0.0186, Val=0.0183\nEpoch 33: Train=0.0185, Val=0.0183 â­ NEW BEST\nEpoch 34: Train=0.0185, Val=0.0183\nEpoch 35: Train=0.0185, Val=0.0183\nEpoch 36: Train=0.0185, Val=0.0183 â­ NEW BEST\n\nâœ… Best Validation Loss: 0.0183\n\n================================================================================\nPREDICTIONS (WITH TEMPERATURE SCALING)\n================================================================================\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Predicting:   0%|          | 0/1109 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"40a1032e2a7e4de298b7daeb6ecce415"}},"metadata":{}},{"name":"stdout","text":"âœ“ Generated 3,363,401 predictions\n\n================================================================================\nENSEMBLE\n================================================================================\nMerging with BLAST...\n  DL=3,363,401, BLAST=11,977,931\nâœ“ Saved 14,090,857 predictions\n\n[CLEANUP]\n  âœ— Removed temp_dl.tsv\n\n================================================================================\nâœ… VERIFICATION\n================================================================================\nðŸ“‹ File: submission.tsv\nðŸ“Š Lines: 14,090,857\nðŸ’¾ Size: 354.4 MB\n\nâœ… First 5 lines:\n  âœ“ A0A009IHW8      GO:0003953   0.6\n  âœ“ A0A009IHW8      GO:0007165   0.6\n  âœ“ A0A009IHW8      GO:0016787   0.6\n  âœ“ A0A009IHW8      GO:0019677   0.6\n  âœ“ A0A009IHW8      GO:0050135   0.6\n\nðŸ” Format check:\n  âœ“ Columns: 3\n  âœ“ GO term format\n  âœ“ Confidence: 0.6\n  âœ“ Newlines present\n\nâœ…âœ…âœ… FORMAT IS CORRECT! âœ…âœ…âœ…\n\nðŸ“ Files in working directory:\n  submission.tsv (354.4 MB)\n\n================================================================================\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}